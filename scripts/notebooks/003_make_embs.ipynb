{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c10062-4aaf-43c5-9a56-0fcc54568679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03dc12a2-26ae-408f-91cd-e701c69f7cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 77\n",
    "# MODEL_NAME = 'BAAI/bge-base-en-v1.5'\n",
    "MODEL_NAME = 'openai/clip-vit-large-patch14'\n",
    "# MODEL_NAME = 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'\n",
    "# MODEL_NAME = 'johngiorgi/declutr-base'\n",
    "# MAX_LENGTH = 88\n",
    "# MODEL_NAME = '../Llama-2-7b-hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1744b-72dc-427d-8256-a5e2499ea129",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ca2e84-26f7-4d4d-bbe7-6adc0df0f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../kcg-ml-image-pipeline/output/dataset/data/environmental/'\n",
    "PMT_PATH = 'data/environmental/prompt.json'\n",
    "EMB_PATH = 'data/environmental/clip_text_emb.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb74eb-e538-42f7-9808-92a8cccfcb3a",
   "metadata": {},
   "source": [
    "## save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a18304-cc24-450b-9c08-92ef4dca5cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-vae-test/'))\n",
    "from utilities.utils import read_embedding_data, read_msg_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d73d781-ae7d-495b-8e33-21e121330aef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c4b50919f94067983cb7e86982a94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = sorted(glob.glob(os.path.join(INPUT_DIR, '**/*_data.msgpack')))\n",
    "\n",
    "file_paths = list()\n",
    "file_hashs = list()\n",
    "positive_prompts = list()\n",
    "negative_prompts = list()\n",
    "creation_times = list()\n",
    "for path in tqdm(paths):\n",
    "    mp = read_msg_pack(path)\n",
    "    file_paths.append(mp['file_path'])\n",
    "    file_hashs.append(mp['file_hash'])\n",
    "    positive_prompts.append(mp['positive_prompt'])\n",
    "    negative_prompts.append(mp['negative_prompt'])\n",
    "    creation_times.append(mp['creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1439c9ae-568d-4bdc-854b-4d066bfcf0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(PMT_PATH)[0], exist_ok=True)\n",
    "\n",
    "json.dump(\n",
    "    pd.DataFrame(zip(positive_prompts, negative_prompts, file_paths, creation_times), columns=['positive_prompt', 'negative_prompt', 'file_path', 'creation_time'], index=file_hashs).to_dict(orient='index'),\n",
    "    open(PMT_PATH, 'w')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71358c40-3a87-451d-9d0b-bee09876493a",
   "metadata": {},
   "source": [
    "## load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c6ed04-ec55-47da-b8fd-d044a044c2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_hashs = list()\n",
    "file_paths = list()\n",
    "positive_prompts = list()\n",
    "negative_prompts = list()\n",
    "\n",
    "for file_hash, info in json.load(open(PMT_PATH)).items():\n",
    "    \n",
    "    file_hashs.append(file_hash)\n",
    "    file_paths.append(info['file_path'])\n",
    "    positive_prompts.append(info['positive_prompt'])\n",
    "    negative_prompts.append(info['negative_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5511b-ddfd-4dc9-92b9-ee9b78d8a193",
   "metadata": {},
   "source": [
    "# load text embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8a0b7-0d43-4032-aa65-63ce99cd2792",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## from kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57186428-fa0c-4312-bd9d-68247824e5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ln -s ../kcg-ml-sd1p4/input/ input\n",
    "# !ln -s ../kcg-ml-sd1p4/output/ output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf1b0a0-0642-4ef0-b37c-b6021ff9a067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mINFO: Created a temporary directory at /tmp/tmpejv92m2d\u001b[0m\n",
      "\u001b[1;32mINFO: Writing /tmp/tmpejv92m2d/_remote_module_non_scriptable.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-sd1p4/'))\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7777febf-a149-4c98-9bc7-69f1c2f17008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and transformer\u001b[32m...[DONE]\u001b[0m\u001b[34m\t2,403.54ms\u001b[0m                             \n"
     ]
    }
   ],
   "source": [
    "clip_text_embedder = CLIPTextEmbedder(device='cuda')\n",
    "_ = clip_text_embedder.load_submodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6317b86-eeb7-4f93-bec6-de9394f2b903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts):\n",
    "    \n",
    "    clip_text_opt = clip_text_embedder.forward_return_all(texts)\n",
    "\n",
    "    last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    attention_mask = clip_text_opt.attention_mask.detach().cpu().numpy()\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79377829-7d62-4df6-9f54-61f12ea33d78",
   "metadata": {},
   "source": [
    "## from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019884b2-42d8-4c74-b41a-9f725b10c7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e2f96-d4c9-4ac0-bca0-f4f75cd98b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### from CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559844df-0456-4fab-87ff-04f59a2749f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).text_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0490001b-a764-4abb-970e-a8c0d1ede92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts, use_penultimate=False):\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "    clip_text_opt = transformer(input_ids=tokens, output_hidden_states=True, return_dict=True)\n",
    "    \n",
    "    attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    \n",
    "    if use_penultimate:\n",
    "        last_hidden_state = clip_text_opt.hidden_states[-1].detach().cpu().numpy()\n",
    "    else:\n",
    "        last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580208c-b733-47bd-af51-9841bea563b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14127512-6084-4737-ac7d-4af8ae09517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker(texts):\n",
    "    \n",
    "    batch_encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True, max_length=MAX_LENGTH, return_length=True,\n",
    "        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "\n",
    "    clip_text_opt = transformer(input_ids=tokens)\n",
    "\n",
    "    last_hidden_state = clip_text_opt.last_hidden_state.detach().cpu().numpy()\n",
    "    attention_mask = batch_encoding.attention_mask.detach().cpu().numpy()\n",
    "    \n",
    "    pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "    # pooler_output = None\n",
    "    \n",
    "    return last_hidden_state, pooler_output, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d8a83a-eda4-46e2-8d0a-7e29194f3d69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /johngiorgi/declutr-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fdb2e5ce7d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 844b863d-ff1b-4646-a032-384b3cb8a040)')' thrown while requesting HEAD https://huggingface.co/johngiorgi/declutr-base/resolve/main/config.json\n",
      "Some weights of the model checkpoint at johngiorgi/declutr-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(MODEL_NAME).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbadf41-a3c2-4bb2-aaa8-f0e8d5c99e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14a1ff-fb5b-42de-8143-447b1761a3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dccc6dafc5a4dcda5c7b144d0d42744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../Llama-2-7b-hf were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "transformer = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, load_in_8bit=True, device_map='auto').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360503-30e0-48d3-b0de-6f3d3ab4b63b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# embed & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52457393-ceb4-46fc-9ef3-c143cc7faf67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b49640eab424aeba7144738d22a3b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59f9a409a944d61b12d1ed60cb810c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_last_hidden_states = list()\n",
    "positive_pooler_outputs = list()\n",
    "positive_attention_masks = list()\n",
    "\n",
    "negative_last_hidden_states = list()\n",
    "negative_pooler_outputs = list()\n",
    "negative_attention_masks = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(0, len(positive_prompts), BATCH_SIZE)):\n",
    "        \n",
    "        last_hidden_state, pooler_output, attention_mask = worker(positive_prompts[i:i+BATCH_SIZE])\n",
    "        \n",
    "        # positive_last_hidden_states.append(last_hidden_state)\n",
    "        # positive_attention_masks.append(attention_mask)\n",
    "        if pooler_output is not None:\n",
    "            positive_pooler_outputs.append(pooler_output)\n",
    "    \n",
    "    for i in tqdm(range(0, len(negative_prompts), BATCH_SIZE)):\n",
    "        \n",
    "        last_hidden_state, pooler_output, attention_mask = worker(negative_prompts[i:i+BATCH_SIZE])\n",
    "        \n",
    "#         negative_last_hidden_states.append(last_hidden_state)\n",
    "#         negative_attention_masks.append(attention_mask)\n",
    "        if pooler_output is not None:\n",
    "            negative_pooler_outputs.append(pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c188adf-636d-48f5-b615-191ec1aa7e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positive_last_hidden_states = np.concatenate(positive_last_hidden_states, axis=0)\n",
    "# positive_attention_masks = np.concatenate(positive_attention_masks, axis=0)\n",
    "if len(positive_pooler_outputs) > 0:\n",
    "    positive_pooler_outputs = np.concatenate(positive_pooler_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d0a4fb0-cf53-46bd-be81-534ce0aaadc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# negative_last_hidden_states = np.concatenate(negative_last_hidden_states, axis=0)\n",
    "# negative_attention_masks = np.concatenate(negative_attention_masks, axis=0)\n",
    "if len(positive_pooler_outputs) > 0:\n",
    "    negative_pooler_outputs = np.concatenate(negative_pooler_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa5afb5c-35ee-40a1-b6e8-e0046efb3738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(EMB_PATH)[0], exist_ok=True)\n",
    "\n",
    "np.savez(\n",
    "    EMB_PATH, \n",
    "    file_hashs=np.array(file_hashs), \n",
    "    file_paths=np.array(file_paths), \n",
    "    # positive_last_hidden_states=positive_last_hidden_states, \n",
    "    # positive_attention_masks=positive_attention_masks,\n",
    "    positive_pooler_outputs=positive_pooler_outputs,\n",
    "    # negative_last_hidden_states=negative_last_hidden_states,\n",
    "    # negative_attention_masks=negative_attention_masks,\n",
    "    negative_pooler_outputs=negative_pooler_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd5a93-65ff-4fb9-a8ff-b4f5200de3be",
   "metadata": {},
   "source": [
    "# vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a233f80b-75af-4ae6-a153-bdc6206aeff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPUT_DIR = '../dataset/civitai-stable-diffusion-337k/images/'\n",
    "# OUTPUT_DIR = '../dataset/civitai-stable-diffusion-337k/clip/'\n",
    "\n",
    "INPUT_DIR = '../dataset/scrap/steam/screenshot/'\n",
    "OUTPUT_DIR = '../dataset/scrap/steam/clip/'\n",
    "\n",
    "# INPUT_DIR = '../dataset/scrap/leonardo/images/'\n",
    "# OUTPUT_DIR = '../dataset/scrap/leonardo/clip/'\n",
    "\n",
    "# INPUT_DIR = '../dataset/midjourney-messages/images/'\n",
    "# OUTPUT_DIR = '../dataset/midjourney-messages/clip/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75016e33-3f73-4f6a-81a6-2dbaa89f557d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ca42ef-3392-4963-a43d-73369118ca2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = CLIPImageProcessor.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(MODEL_NAME, local_files_only=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced115f8-2ba4-48bb-bb24-70425cf6ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = list()\n",
    "\n",
    "for file_name in os.listdir(INPUT_DIR):\n",
    "    if not file_name.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "        continue\n",
    "    clip_path = os.path.join(OUTPUT_DIR, f'{os.path.splitext(file_name)[0]}.npy')\n",
    "    \n",
    "    if os.path.exists(clip_path):\n",
    "        continue\n",
    "        \n",
    "    file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05d49ba-54bd-4172-9ee3-43573a08aa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621e257b83b742358f5b065518f7ceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(file_names), BATCH_SIZE)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = list()\n",
    "        names = list()\n",
    "        for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "            try:\n",
    "                image = Image.open(os.path.join(INPUT_DIR, file_name))\n",
    "                image = preprocessor(images=image, return_tensors=\"pt\")\n",
    "            except:\n",
    "                continue\n",
    "            images.append(image['pixel_values'])\n",
    "            names.append(file_name)\n",
    "\n",
    "        images = torch.concat(images, dim=0)\n",
    "    \n",
    "        image_features = clip_model.get_image_features(pixel_values=images.to(clip_model.device))\n",
    "        image_features = image_features.detach().cpu().numpy()\n",
    "        \n",
    "        for file_name, image_feature in zip(names, image_features):\n",
    "            clip_path = os.path.join(OUTPUT_DIR, f'{os.path.splitext(file_name)[0]}.npy')\n",
    "            np.save(clip_path, image_feature[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19440699-5b26-40d8-8eac-a6e7b361c564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
